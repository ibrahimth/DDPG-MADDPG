{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99 finished with -1158.482141802852 total rewards, T: 20000\n",
      "Episode 199 finished with -959.8251699680808 total rewards, T: 40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76009d2d0d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0mN_EPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;31m# rewards_DQN_dueling = learn_episodic_DQN(N_EPS, 500, use_dueling=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m \u001b[0mrewards_DDPG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_episodic_DDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_DDPG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DDPG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-76009d2d0d81>\u001b[0m in \u001b[0;36mlearn_episodic_DDPG\u001b[0;34m(N_eps)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# train the DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-76009d2d0d81>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(memory, batch_size, df, T)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m#     writer.add_scalar(\"Actor loss\", loss_actor.item(), T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mloss_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0moptim_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_value_net_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_value_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gym/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def moving_average(x, N):\n",
    "    return np.convolve(x, np.ones(N, ), mode='valid') / N\n",
    "\n",
    "# taken from openAI baselines\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n",
    "class Policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_net, self).__init__()\n",
    "        self.affine1 = nn.Linear(3, 200)\n",
    "        self.affine2 = nn.Linear(200, 100)\n",
    "        self.mean_head = nn.Linear(100, 1)\n",
    "        # self.sigma = torch.nn.Parameter(torch.tensor([self.sigma], requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = F.relu(self.affine2(x))\n",
    "        # -2 to 2 with tanh\n",
    "        mean = 2*torch.tanh(self.mean_head(x))\n",
    "        return  mean\n",
    "\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        action_space = 1\n",
    "        self.affine1 = nn.Linear(3, 200)\n",
    "        self.affine2 = nn.Linear(200, 100)\n",
    "        self.value_head = nn.Linear(100, 10)\n",
    "        self.value_head2 = nn.Linear(action_space + 10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        action, state  = x\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = F.relu(self.affine2(x))\n",
    "        x = F.relu(self.value_head(x))\n",
    "        x = torch.cat([x, action])\n",
    "        return self.value_head2(x)\n",
    "    \n",
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L11\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L15\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "        \n",
    "Q_value_net = Q_net()\n",
    "Q_value_net_target = Q_net()\n",
    "hard_update(Q_value_net_target, Q_value_net)\n",
    "\n",
    "policy_net = Policy_net()\n",
    "policy_net_target = Policy_net()\n",
    "hard_update(policy_net_target, policy_net)\n",
    "\n",
    "Q_value_net_target.eval()\n",
    "policy_net_target.eval()\n",
    "\n",
    "optim_q = optim.Adam(Q_value_net.parameters(), lr=3e-4)\n",
    "optim_p = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
    "\n",
    "# def select_greedy(obs):\n",
    "#     with torch.no_grad():\n",
    "#         obs_ = torch.from_numpy(obs).float()\n",
    "#         values = Q_network(obs_)\n",
    "#         return torch.argmax(values.detach()).view(1, -1)\n",
    "    \n",
    "# def get_action(observation):\n",
    "#     x = torch.from_numpy(observation).float()\n",
    "#     normal = policy_net(x)\n",
    "#     return normal.sample()\n",
    "\n",
    "def get_q_inputs(state, action):\n",
    "    return torch.cat([state, action], 1)\n",
    "\n",
    "def train_on_batch(memory, batch_size, df, T):\n",
    "    # TODO-in future: remove the casting to tensors all the time\n",
    "    # Vectorized implementation\n",
    "    batch = memory.sample(batch_size)\n",
    "    # connect all batch Transitions to one tuple\n",
    "    batch_n = Transition(*zip(*batch))\n",
    "    # reshape actions so ve can collect the DQN(S_t, a_t) easily with gather\n",
    "    actions = torch.tensor(batch_n.action).float().view(-1, 1)\n",
    "    # get batch states\n",
    "#     print(batch_n.state)\n",
    "    states = torch.cat(batch_n.state).float()\n",
    "    next_states = torch.cat(batch_n.next_state).float()\n",
    "    batch_rewards = torch.cat(batch_n.reward).float().view(-1, 1)\n",
    "    \n",
    "    dones = torch.tensor(batch_n.done).float().view(-1, 1)\n",
    "    # collect only needed Q-values with corresponding actions for loss computation\n",
    "    inputs = Q_value_net(get_q_inputs(states, actions))\n",
    "    targets = batch_rewards\n",
    "    # targets += df*Q_network_target(next_states).max(1)[0].detach()*(1 - dones)\n",
    "    targets += (1-dones)*df*Q_value_net_target(torch.cat([next_states, policy_net_target(next_states).view(-1, 1)], 1))\n",
    "    \n",
    "    # critic loss\n",
    "    optim_q.zero_grad()\n",
    "    loss = F.mse_loss(inputs, targets.view(inputs.shape))\n",
    "#     writer.add_scalar(\"Critic loss\", loss.item(), T)\n",
    "    loss.backward()\n",
    "    optim_q.step()\n",
    "\n",
    "    # actor loss\n",
    "    optim_p.zero_grad()\n",
    "    loss_actor = -1 * (Q_value_net(torch.cat([states, policy_net(states).view(-1, 1)], 1))).mean()\n",
    "#     writer.add_scalar(\"Actor loss\", loss_actor.item(), T)\n",
    "    loss_actor.backward()\n",
    "    optim_q.step()\n",
    "\n",
    "    soft_update(Q_value_net_target, Q_value_net, tau=0.001)\n",
    "    soft_update(policy_net_target, policy_net, tau=0.001)\n",
    "    \n",
    "def learn_episodic_DDPG(N_eps=500): \n",
    "    \n",
    "    memory_len = 1000000\n",
    "    df = 0.99\n",
    "    batch_size = 64\n",
    "    train_freq = 16\n",
    "    T = 0\n",
    "    # target_update_freq = 1000\n",
    "    warmup_steps = 10000\n",
    "    # scheduler\n",
    "    e_s = 1.5\n",
    "    e_e = 0.10\n",
    "    N_decay = 80000\n",
    "    scheduler = LinearSchedule(N_decay, e_e, e_s)\n",
    "    \n",
    "    # replay mem\n",
    "    memory = ReplayMemory(memory_len)\n",
    "    rewards = []\n",
    "#     writer = SummaryWriter()\n",
    "\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    # n_actions = env.action_space.n\n",
    "    actions = []\n",
    "    for i_episode in range(N_eps):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        total_r = 0\n",
    "\n",
    "        for t in range(200):\n",
    "            T += 1\n",
    "            if T < warmup_steps:\n",
    "                action = env.action_space.sample()[0]\n",
    "            else:\n",
    "                curr_epsilon = scheduler.value(T - warmup_steps)\n",
    "                noise = np.random.normal(0, curr_epsilon)\n",
    "                action_mean = policy_net(torch.from_numpy(observation).float())\n",
    "                action = np.clip(action_mean.item() + noise , -2.0, 2.0)\n",
    "\n",
    "                # print(action)\n",
    "            next_observation, reward, done, info = env.step([action])\n",
    "            total_r += reward\n",
    "            reward = torch.tensor([reward])\n",
    "            \n",
    "            memory.push(torch.from_numpy(observation).view(1, -1), \\\n",
    "                action, reward, torch.from_numpy(next_observation).view(1, -1), float(done))\n",
    "            \n",
    "            # train the DQN\n",
    "            if T % train_freq == 0:\n",
    "                train_on_batch(memory, min(batch_size, T), df, T)\n",
    "\n",
    "            observation = next_observation\n",
    "                \n",
    "            if done:\n",
    "#                 print(done)\n",
    "#                 writer.add_scalar(\"Episode_reward\", total_r, i_episode)\n",
    "                if (i_episode + 1) % 100 == 0:\n",
    "                    # print('curr eps', curr_epsilon)\n",
    "                    print(\"Episode {} finished with {} total rewards, T: {}\".format(i_episode, total_r, T))\n",
    "                \n",
    "        rewards.append(total_r)\n",
    "\n",
    "    # render environment\n",
    "    for i in range(5):\n",
    "        observation = env.reset()\n",
    "        for j in range(500):\n",
    "            action_mean = policy_net(torch.from_numpy(observation).float())\n",
    "            action = np.clip(action_mean.item(), -2.0, 2.0)\n",
    "            next_observation, reward, done, info = env.step([action])\n",
    "            env.render()\n",
    "    env.close()\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "N_EPS = 500\n",
    "# rewards_DQN_dueling = learn_episodic_DQN(N_EPS, 500, use_dueling=True)\n",
    "rewards_DDPG = learn_episodic_DDPG(N_EPS)\n",
    "plt.plot(moving_average(rewards_DDPG, 100), label=\"DDPG\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
